1.任务介绍
需求分析
爬取豆瓣电影Top250的基本信息，包括电影的名称、豆瓣评分、评价数、电影概况、电影链接等
https://movie.douban.com/top250

2.爬虫初识
.什么是爬虫？
    网络爬虫，是一种安装一定【规则，自动抓取】互联网信息的程序或者脚本。
由于互联网数据的多样性和资源的有限性，根据【用户需求定向】抓取相关网页并分析已成为如今主流的爬取策略
.爬虫可以做什么？
    你可以爬取妹子的图片，爬取自己想看的视频等待，只要你能通过浏览器访问的数据都可以通过爬虫获取
.爬虫的本质是什么？
    模拟浏览器打开网页，获取网页中我们想要的那部分数据

3.基本流程
.准备工作
  通过浏览器查看分析目标网页，学习编程基础规范
.获取数据  urlib2
  通过HTTP库向目标站点发起请求，请求可以包含额外的header等信息，
如果服务器能正常响应，会得到一个Response，便是所要获取的页面内容
.解析内容
  得到的内容可能是HTML,json等格式，可以用页面解析库、正则表达式等进行解析
.保存数据
 保存形式多样，可以存为文本，也可以保存到数据库，或者保存到特定格式的文件

3.1准备工作
 .URL分析
   页面包括250条电影数据，分10页，每页25条
   每页的URL的不同之处：最后的数值=（页数-1）*25
 3.1.1分析页面   F12   【Elements+Network】
        Headers
        借助Chrome开发者工具（F12）来分析网页，在Elements下找到需要的数据位置
        User-Agent  登陆后才能爬取Cookie
 3.1.2编码规范
 一般第一行需要加入# -*-coding:utf-8-*-或者coding=utf-8  保证文字在中文处理是正确的
 使用函数是实现单一功能或相关联功能的代码段，提高可读性贺代码复用率
 加入main函数用于测试   if __name__=="__main__":
 # 加注释
 3.1.3引入模块
 模块：用来从逻辑上组织Python代码（变量、函数、类），本质就是py文件，提高代码的可维护性。
 Pyhon使用import 来导入模块
 import sys 系统模块
 from bs4 import BeautifulSoup
 import re
 import urllib
 import xlwt

3.2获取数据
  一般使用urllib库获取页面
  获取页面数据
    对每一个页面,调用askURL函数获取页面内容
    定义一个获取页面的函数askURL,传入一个url参数,表示网址,如https://movie.douban.com/top250?start=
    urllib.Request生成请求:
    urllib.urlopen发送请求响应;read获取页面内容
  在访问页面时经常或出现错误,为了程序正常运行,加入异常捕获try...except..语句
 3.3解析内容
 对爬取的html文件进行解析
 解析页面内容
    使用BeautifulSoup定位特定的标签位置
    使用正则表达式找到具体的内容

    BeatifulSoup4将复杂的HTML转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为4种：
    -Tag
    -NavigableString
    -BeautifulSoup
    -Comment

 3.3.1 标签解析
    BeautifulSoup是一个库,提供一些简单的\python式的用来处理导航\搜索\修改分析树等功能,
  通过解析文档为用户提供需要抓取的数据.
  创建BeautifulSoup对象,html为页面内容,html.parser是一种页面解析器
 3.3.2正则提取
    正则表达式,通常被用来检索\替换那些符合某个模式(规则)的文本
    正则表达式:字符串模式（判断字符串是否符合一定的标准）
    re

3.4 保存数据
 3.4.1 Excel表存储
    Excel表格存储
    利用python库xlwt将抽取的数据datalist写入Excel表格